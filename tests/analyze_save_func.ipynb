{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814130a1",
   "metadata": {},
   "source": [
    "# Analysis of Save Functions - Optimization Opportunities\n",
    "\n",
    "## Functions Analyzed\n",
    "1. `save_processed_data_by_weekday_to_dataframe()`\n",
    "2. `save_stations_by_category()`\n",
    "\n",
    "## Current Performance\n",
    "- **Total runtime**: ~1 full day (24 hours) to process and save all station data\n",
    "- This is unacceptable for production use and iterative development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634eacd",
   "metadata": {},
   "source": [
    "## Function 1: `save_stations_by_category()` - Batch Processing Orchestrator\n",
    "\n",
    "### Current Implementation Analysis\n",
    "\n",
    "#### ‚úÖ Already Optimized:\n",
    "1. **Pre-loads all data once** - Schedule and incident data are loaded once and reused for all stations\n",
    "2. **Avoids redundant file I/O** - No reloading for each station\n",
    "3. **Sequential processing** - Processes stations one by one\n",
    "\n",
    "#### ‚ö†Ô∏è Major Bottlenecks:\n",
    "\n",
    "### 1. **NO PARALLEL PROCESSING**\n",
    "- **Current**: Sequential loop through all stations (one at a time)\n",
    "- **Problem**: Only uses 1 CPU core out of potentially 8-16 available\n",
    "- **Impact**: Linear scaling - processing 100 stations takes 100x longer than 1 station\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Instead of:\n",
    "for st_code in stations:\n",
    "    process_single_station(st_code)\n",
    "\n",
    "# Use parallel processing:\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "# Create worker function\n",
    "worker_func = partial(process_single_station, \n",
    "                      schedule_data=schedule_data_loaded,\n",
    "                      stanox_ref=stanox_ref,\n",
    "                      tiploc_to_stanox=tiploc_to_stanox,\n",
    "                      incident_data=incident_data_loaded)\n",
    "\n",
    "# Process in parallel with 8 workers\n",
    "with Pool(processes=8) as pool:\n",
    "    results = pool.map(worker_func, stations)\n",
    "```\n",
    "\n",
    "**Expected speedup**: 6-8x faster (on 8-core machine)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Inefficient File Cleanup**\n",
    "- **Current**: Uses `shutil.rmtree()` for EACH station folder individually\n",
    "- **Problem**: Lots of filesystem operations in a loop\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Instead of removing each station folder individually,\n",
    "# remove the entire processed_data directory once:\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "```\n",
    "\n",
    "**Expected speedup**: Marginal, but cleaner code\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Progress Tracking Overhead**\n",
    "- **Current**: Extensive print statements for every operation\n",
    "- **Problem**: Console I/O can slow down processing, especially when running hundreds of stations\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Use logging with levels instead of print statements\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # Can be changed to WARNING for production\n",
    "\n",
    "# Or use tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "for st_code in tqdm(stations, desc=\"Processing stations\"):\n",
    "    # process...\n",
    "```\n",
    "\n",
    "**Expected speedup**: 5-10% when processing many stations\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **No Checkpointing/Resume Capability**\n",
    "- **Current**: If processing crashes at station 80/100, you start over from station 1\n",
    "- **Problem**: Wastes time re-processing already completed stations\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Check which stations are already processed\n",
    "completed_stations = []\n",
    "for st_code in stations:\n",
    "    station_folder = os.path.join(output_dir, st_code)\n",
    "    if os.path.exists(station_folder) and has_all_weekday_files(station_folder):\n",
    "        completed_stations.append(st_code)\n",
    "\n",
    "# Only process remaining stations\n",
    "remaining_stations = [s for s in stations if s not in completed_stations]\n",
    "print(f\"Skipping {len(completed_stations)} already completed stations\")\n",
    "```\n",
    "\n",
    "**Expected benefit**: Fault tolerance - can resume after crashes\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Memory Management**\n",
    "- **Current**: Loads ALL schedule data (potentially GB) and ALL incident data into memory\n",
    "- **Problem**: May cause memory issues on machines with limited RAM\n",
    "\n",
    "**Note**: This is actually necessary for the optimization strategy (load once, reuse), so it's a trade-off.\n",
    "\n",
    "**Alternative Approach** (if memory is a problem):\n",
    "- Process in batches of stations (e.g., 50 at a time)\n",
    "- Load data ‚Üí Process 50 stations ‚Üí Clear memory ‚Üí Repeat\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c453f6",
   "metadata": {},
   "source": [
    "## Function 2: `save_processed_data_by_weekday_to_dataframe()` - Single Station Processor\n",
    "\n",
    "### Current Implementation Analysis\n",
    "\n",
    "#### ‚úÖ Already Optimized:\n",
    "1. **Accepts pre-loaded data** - Avoids redundant file I/O when called in batch mode\n",
    "2. **Uses optimized delay processing** - `process_delays_optimized()` instead of legacy version\n",
    "\n",
    "#### ‚ö†Ô∏è Major Bottlenecks:\n",
    "\n",
    "### 1. **Deduplication Algorithm is O(n¬≤) in worst case**\n",
    "- **Current**: Nested loop creating hashable tuples for each entry\n",
    "- **Problem**: For stations with 10,000+ entries, this becomes very slow\n",
    "\n",
    "**Current Code:**\n",
    "```python\n",
    "seen = set()\n",
    "deduplicated = []\n",
    "for entry in schedule_timeline_adjusted:\n",
    "    key_fields = []\n",
    "    for k, v in sorted(entry.items()):  # <-- Sorting every item for every entry\n",
    "        if isinstance(v, (str, int, float, type(None))):\n",
    "            key_fields.append((k, v))\n",
    "        elif isinstance(v, list):\n",
    "            key_fields.append((k, tuple(v)))\n",
    "    entry_key = tuple(key_fields)\n",
    "    if entry_key not in seen:\n",
    "        seen.add(entry_key)\n",
    "        deduplicated.append(entry)\n",
    "```\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Use pandas for deduplication (MUCH faster)\n",
    "df_temp = pd.DataFrame(schedule_timeline_adjusted)\n",
    "\n",
    "# Convert list columns to hashable strings for deduplication\n",
    "list_cols = [col for col in df_temp.columns if df_temp[col].apply(lambda x: isinstance(x, list)).any()]\n",
    "for col in list_cols:\n",
    "    df_temp[col] = df_temp[col].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Use pandas drop_duplicates (optimized C implementation)\n",
    "df_dedup = df_temp.drop_duplicates()\n",
    "\n",
    "# Convert back to list of dicts\n",
    "schedule_timeline_adjusted = df_dedup.to_dict('records')\n",
    "```\n",
    "\n",
    "**Expected speedup**: 10-50x faster for large datasets\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Weekday Organization Creates Unnecessary Copies**\n",
    "- **Current**: Manually loops through entries and creates copies for multi-day schedules\n",
    "- **Problem**: Uses `.copy()` extensively which is slow\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Instead of looping and copying:\n",
    "# Use pandas explode() to expand multi-day schedules efficiently\n",
    "df = pd.DataFrame(schedule_timeline_adjusted)\n",
    "\n",
    "# Explode ENGLISH_DAY_TYPE to create one row per day\n",
    "df_expanded = df.explode('ENGLISH_DAY_TYPE')\n",
    "df_expanded['WEEKDAY'] = df_expanded['ENGLISH_DAY_TYPE']\n",
    "df_expanded['DATASET_TYPE'] = df_expanded['ENGLISH_DAY_TYPE'].apply(\n",
    "    lambda x: 'SINGLE_DAY' if len(x) == 1 else 'MULTI_DAY'\n",
    ")\n",
    "\n",
    "# Group by WEEKDAY\n",
    "weekday_dataframes = {day: group for day, group in df_expanded.groupby('WEEKDAY')}\n",
    "```\n",
    "\n",
    "**Expected speedup**: 5-10x faster for this step\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Custom Sorting Function is Inefficient**\n",
    "- **Current**: Custom `safe_sort_key()` function called for every entry\n",
    "\n",
    "**Current Code:**\n",
    "```python\n",
    "def safe_sort_key(x):\n",
    "    actual_calls = x.get(\"ACTUAL_CALLS\")\n",
    "    if actual_calls is None or actual_calls == \"NA\":\n",
    "        return 0\n",
    "    # ... lots of type checking\n",
    "    \n",
    "entries.sort(key=safe_sort_key)\n",
    "```\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Use pandas sorting (faster, handles type conversion automatically)\n",
    "df['ACTUAL_CALLS_NUMERIC'] = pd.to_numeric(df['ACTUAL_CALLS'], errors='coerce').fillna(0)\n",
    "df = df.sort_values('ACTUAL_CALLS_NUMERIC')\n",
    "df = df.drop(columns=['ACTUAL_CALLS_NUMERIC'])  # Remove temp column\n",
    "```\n",
    "\n",
    "**Expected speedup**: 3-5x faster for this step\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Excessive Print Statements**\n",
    "- **Current**: 6 print statements per station, each with string formatting\n",
    "- **Problem**: When processing 200+ stations, this adds up\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Use a verbosity flag\n",
    "def save_processed_data_by_weekday_to_dataframe(st_code, verbose=True, ...):\n",
    "    if verbose:\n",
    "        print(f\"Processing data for STANOX: {st_code}\")\n",
    "    # ...\n",
    "\n",
    "# When called in batch mode, set verbose=False\n",
    "# Only log major milestones or errors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Parquet Writing Could Be Optimized**\n",
    "- **Current**: Writes each weekday file individually with default settings\n",
    "\n",
    "**Optimization Opportunity:**\n",
    "```python\n",
    "# Use compression and optimized engine\n",
    "df.to_parquet(\n",
    "    filename, \n",
    "    index=False,\n",
    "    engine='pyarrow',  # Faster than fastparquet\n",
    "    compression='snappy'  # Good balance of speed and size\n",
    ")\n",
    "```\n",
    "\n",
    "**Expected benefit**: Smaller file sizes, potentially faster writes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34f21d",
   "metadata": {},
   "source": [
    "## Summary: Optimization Priority Ranking\n",
    "\n",
    "### üî¥ **CRITICAL - Implement First** (Expected 6-10x speedup)\n",
    "1. **Parallel processing in `save_stations_by_category()`**\n",
    "   - Use `multiprocessing.Pool` with 8 workers\n",
    "   - Expected speedup: 6-8x (24 hours ‚Üí 3-4 hours)\n",
    "\n",
    "### üü° **HIGH PRIORITY** (Expected 3-5x additional speedup)\n",
    "2. **Replace deduplication with pandas in `save_processed_data_by_weekday_to_dataframe()`**\n",
    "   - Expected speedup: 10-50x for this step\n",
    "   \n",
    "3. **Replace weekday organization logic with pandas operations**\n",
    "   - Expected speedup: 5-10x for this step\n",
    "\n",
    "### üü¢ **MEDIUM PRIORITY** (Expected 10-20% additional speedup)\n",
    "4. **Reduce print statements / use logging levels**\n",
    "5. **Optimize sorting with pandas**\n",
    "6. **Add checkpointing/resume capability**\n",
    "\n",
    "### üîµ **LOW PRIORITY** (Marginal improvements)\n",
    "7. **Optimize file cleanup**\n",
    "8. **Add parquet compression settings**\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Expected Improvement\n",
    "\n",
    "**Current**: ~24 hours\n",
    "\n",
    "**After Critical + High Priority optimizations**:\n",
    "- Parallel processing: 6x speedup ‚Üí 4 hours\n",
    "- Pandas-based deduplication/organization: 3x speedup ‚Üí 1.3 hours\n",
    "- Other optimizations: 1.2x speedup ‚Üí **~1 hour total**\n",
    "\n",
    "**Expected final runtime: 1-2 hours** (from 24 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916729e",
   "metadata": {},
   "source": [
    "## Additional Considerations\n",
    "\n",
    "### Memory Usage\n",
    "- Pre-loading all data is memory-intensive but necessary for speed\n",
    "- If memory becomes an issue, consider batch processing (50 stations at a time)\n",
    "- Monitor memory usage with `memory_profiler` package\n",
    "\n",
    "### Data Validation\n",
    "- Currently no validation that parquet files were written successfully\n",
    "- Consider adding file existence/size checks after writing\n",
    "\n",
    "### Error Handling\n",
    "- Current error handling catches exceptions and continues\n",
    "- With parallel processing, need to ensure errors in one worker don't crash entire job\n",
    "- Consider using `try/except` within worker functions\n",
    "\n",
    "### Testing\n",
    "- Before implementing optimizations, create tests to verify output remains identical\n",
    "- Test with a small subset of stations first (e.g., 5 stations)\n",
    "- Compare output files byte-by-byte to ensure correctness\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a269d",
   "metadata": {},
   "source": [
    "## Implementation Roadmap\n",
    "\n",
    "### Phase 1: Parallel Processing (Week 1)\n",
    "1. Refactor `save_processed_data_by_weekday_to_dataframe()` to be process-safe\n",
    "2. Implement multiprocessing in `save_stations_by_category()`\n",
    "3. Test with 10 stations, then 50, then all\n",
    "4. Measure speedup\n",
    "\n",
    "### Phase 2: Pandas Optimizations (Week 2)\n",
    "1. Replace deduplication logic with pandas\n",
    "2. Replace weekday organization with pandas explode/groupby\n",
    "3. Replace custom sorting with pandas\n",
    "4. Validate outputs match original implementation\n",
    "\n",
    "### Phase 3: Polish (Week 3)\n",
    "1. Add logging levels\n",
    "2. Add checkpointing\n",
    "3. Add progress bars with tqdm\n",
    "4. Optimize parquet writing settings\n",
    "5. Add comprehensive error handling\n",
    "\n",
    "### Phase 4: Validation & Deployment (Week 4)\n",
    "1. Run full test suite\n",
    "2. Compare outputs with original implementation\n",
    "3. Measure final speedup\n",
    "4. Document changes\n",
    "5. Deploy optimized version\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d108ed",
   "metadata": {},
   "source": [
    "‚úÖ Test Results Summary\n",
    "New Save Function Tests (test_save_functions.py)\n",
    "Result: 15/15 PASSED ‚ú®\n",
    "\n",
    "All the new tests for the save functions work correctly:\n",
    "\n",
    "‚úÖ save_processed_data_by_weekday_to_dataframe() - 6 tests\n",
    "\n",
    "Returns dict of DataFrames\n",
    "\n",
    "Returns None when no schedule data\n",
    "\n",
    "Deduplication works correctly\n",
    "\n",
    "Multi-day schedules create entries for all days\n",
    "\n",
    "DataFrames have correct columns\n",
    "\n",
    "Entries are sorted by ACTUAL_CALLS\n",
    "\n",
    "‚úÖ save_stations_by_category() - 5 tests\n",
    "\n",
    "Returns summary dict\n",
    "\n",
    "Returns None when no stations found\n",
    "\n",
    "Handles processing failures gracefully\n",
    "\n",
    "Creates parquet files for each day\n",
    "\n",
    "Cleans up existing station folders\n",
    "\n",
    "‚úÖ load_stations() - 2 tests\n",
    "\n",
    "Filters by category correctly\n",
    "Excludes empty categories when loading all\n",
    "‚úÖ Baseline comparison - 2 tests (placeholder stubs)\n",
    "\n",
    "Note: The 2 errors for TestPerformanceBenchmarks are expected - they require the optional pytest-benchmark plugin which isn't installed. These are for performance measurement only."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
