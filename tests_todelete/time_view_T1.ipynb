{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d616eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_parquet(file_path):\n",
    "    \"\"\"\n",
    "    Safely read parquet files with different engines to handle compatibility issues.\n",
    "    \"\"\"\n",
    "    engines = ['pyarrow', 'fastparquet']\n",
    "    \n",
    "    for engine in engines:\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path, engine=engine)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # If all engines fail, try with auto engine\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame()  # Return empty DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81578431",
   "metadata": {},
   "source": [
    "### This is the first test to visualize a \"time\" view,\n",
    "\n",
    "This view should show the general network delay impacts by inputting a date.\n",
    "\n",
    "---\n",
    "This was done as multiple incidents can occur at once, so providing a general view of the network on a specific date the user wants to analyze can be a helpful resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb8a8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import folium\n",
    "\n",
    "# Add the parent directory to the path to import data modules\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from data.reference import reference_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd5b4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_station_names():\n",
    "    \"\"\"Load station names from the DFT categories reference file.\"\"\"\n",
    "    try:\n",
    "        with open(reference_files[\"all dft categories\"], 'r') as f:\n",
    "            stations_data = json.load(f)\n",
    "        \n",
    "        # Create a mapping from stanox to station name\n",
    "        stanox_to_name = {}\n",
    "        for station in stations_data:\n",
    "            if 'stanox' in station and 'station_name' in station:\n",
    "                stanox_to_name[str(station['stanox'])] = station['station_name']\n",
    "        \n",
    "        return stanox_to_name\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load station reference data: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6efbb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10450237 rows from 2599 files. Skipped 0 files.\n"
     ]
    }
   ],
   "source": [
    "# Load all .parquet files from processed_data into a pandas DataFrame, trying both pyarrow and fastparquet engines\n",
    "\n",
    "data_dir = '../processed_data' if not os.path.isdir('processed_data') else 'processed_data'\n",
    "all_parquet_files = glob.glob(os.path.join(data_dir, '*', '*.parquet'))\n",
    "\n",
    "list_df = []\n",
    "skipped_files = []\n",
    "for file in all_parquet_files:\n",
    "    try:\n",
    "        df = pd.read_parquet(file)\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            df = pd.read_parquet(file, engine='fastparquet')\n",
    "        except Exception as e2:\n",
    "            skipped_files.append(file)\n",
    "            continue\n",
    "    stanox = os.path.basename(os.path.dirname(file))\n",
    "    day = os.path.splitext(os.path.basename(file))[0]\n",
    "    df['STANOX'] = stanox\n",
    "    df['DAY'] = day\n",
    "    list_df.append(df)\n",
    "\n",
    "if list_df:\n",
    "    all_data = pd.concat(list_df, ignore_index=True)\n",
    "    print(f\"Loaded {len(all_data)} rows from {len(list_df)} files. Skipped {len(skipped_files)} files.\")\n",
    "else:\n",
    "    all_data = pd.DataFrame()\n",
    "    print(\"No data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c4e8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_view_html(date_str):\n",
    "    \"\"\"\n",
    "    Create an HTML map showing affected stations for a given date, with markers sized by incident count and colored by total PFPI minutes.\n",
    "    \"\"\"\n",
    "    # --- Color grading function to match incident_view_heatmap_html legend ---\n",
    "    def get_color(delay):\n",
    "        try:\n",
    "            d = float(delay)\n",
    "        except Exception:\n",
    "            d = 0\n",
    "        if d == 0:\n",
    "            return \"blue\"\n",
    "        if d <= 5:\n",
    "            return '#32CD32'     # Minor (1-5 min) - Lime Green\n",
    "        elif d <= 15:\n",
    "            return '#FFD700'     # Moderate (6-15 min) - Gold\n",
    "        elif d <= 30:\n",
    "            return '#FF8C00'     # Significant (16-30 min) - Dark Orange\n",
    "        elif d <= 60:\n",
    "            return '#FF0000'     # Major (31-60 min) - Red\n",
    "        elif d <= 120:\n",
    "            return '#8B0000'     # Severe (61-120 min) - Dark Red\n",
    "        else:\n",
    "            return '#8A2BE2'     # Critical (120+ min) - Blue Violet\n",
    "    \n",
    "    # Filter data for the specified date\n",
    "    filtered_data = all_data[all_data['INCIDENT_START_DATETIME'].str.contains(date_str, na=False)]\n",
    "    \n",
    "    if filtered_data.empty:\n",
    "        print(f\"No data found for date {date_str}\")\n",
    "        return\n",
    "    \n",
    "    # Get unique affected STANOX codes\n",
    "    affected_stanox = filtered_data['STANOX'].unique()\n",
    "    \n",
    "    # Count incidents per STANOX\n",
    "    incident_counts = filtered_data.groupby('STANOX')['INCIDENT_NUMBER'].nunique()\n",
    "    \n",
    "    # Sum PFPI_MINUTES per STANOX\n",
    "    total_pfpi = filtered_data.groupby('STANOX')['PFPI_MINUTES'].sum()\n",
    "    \n",
    "    # Load station coordinates\n",
    "    try:\n",
    "        with open(reference_files[\"all dft categories\"], 'r') as f:\n",
    "            stations_data = json.load(f)\n",
    "        stanox_to_coords = {}\n",
    "        for station in stations_data:\n",
    "            if 'stanox' in station and 'latitude' in station and 'longitude' in station:\n",
    "                stanox_to_coords[str(station['stanox'])] = [station['latitude'], station['longitude']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading coordinates: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create Folium map centered on UK\n",
    "    m = folium.Map(location=[54.5, -2.5], zoom_start=6)\n",
    "    \n",
    "    # Add markers for each affected station, sized by incident count and colored by total PFPI\n",
    "    for stanox in affected_stanox:\n",
    "        stanox_str = str(stanox)\n",
    "        if stanox_str in stanox_to_coords:\n",
    "            lat, lon = stanox_to_coords[stanox_str]\n",
    "            count = incident_counts.get(stanox, 0)\n",
    "            count = int(count) if pd.notna(count) else 0\n",
    "            total_delay = total_pfpi.get(stanox, 0)\n",
    "            total_delay = float(total_delay) if pd.notna(total_delay) else 0.0\n",
    "            color = get_color(total_delay)\n",
    "            radius = int(5 + count * 2)  # Scale radius with incident count\n",
    "            folium.CircleMarker(\n",
    "                location=[float(lat), float(lon)],\n",
    "                radius=radius,\n",
    "                color=color,\n",
    "                fill=True,\n",
    "                fill_color=color,\n",
    "                fill_opacity=0.7,\n",
    "                popup=f\"STANOX: {stanox_str}<br>Incidents: {count}<br>Total Delay: {total_delay:.1f} min\"\n",
    "            ).add_to(m)\n",
    "        else:\n",
    "            print(f\"Coordinates not found for STANOX: {stanox_str}\")\n",
    "    \n",
    "    # Add legend\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; bottom: 50px; left: 50px; width: 180px; height: 180px; background-color: white; border:2px solid grey; z-index:9999; font-size:14px; padding: 10px;\">\n",
    "    <p><b>Delay Legend (Total PFPI Minutes)</b></p>\n",
    "    <p><span style=\"color:blue;\">●</span> 0 min</p>\n",
    "    <p><span style=\"color:#32CD32;\">●</span> 1-5 min</p>\n",
    "    <p><span style=\"color:#FFD700;\">●</span> 6-15 min</p>\n",
    "    <p><span style=\"color:#FF8C00;\">●</span> 16-30 min</p>\n",
    "    <p><span style=\"color:#FF0000;\">●</span> 31-60 min</p>\n",
    "    <p><span style=\"color:#8B0000;\">●</span> 61-120 min</p>\n",
    "    <p><span style=\"color:#8A2BE2;\">●</span> 120+ min</p>\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Save the map to HTML file\n",
    "    output_file = f\"time_view_{date_str.replace('-', '_')}.html\"\n",
    "    m.save(output_file)\n",
    "    print(f\"Map saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1b96da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to time_view_28_APR_2024.html\n"
     ]
    }
   ],
   "source": [
    "create_time_view_html('28-APR-2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd4f28d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to time_view_31_JAN_2024.html\n"
     ]
    }
   ],
   "source": [
    "create_time_view_html('31-JAN-2024')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
