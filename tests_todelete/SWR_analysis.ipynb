{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is meant to analyze the SWR passenger loadings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6d095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11e1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from data.passenger_loadings import SWR_passenger_loadings_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "385bd9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rail Period',\n",
       " 'TrainID',\n",
       " 'Headcode',\n",
       " 'Train UID',\n",
       " 'Origin',\n",
       " 'Destination',\n",
       " 'Origin Time',\n",
       " 'Destination Time',\n",
       " 'Calendar Day',\n",
       " 'Location',\n",
       " 'Location Name',\n",
       " 'Arr',\n",
       " 'Dep',\n",
       " 'Formation',\n",
       " 'Car Count',\n",
       " 'Standard Seated',\n",
       " 'Standing',\n",
       " 'Capacity',\n",
       " 'First Class Seats',\n",
       " 'Average Train Load on Departure',\n",
       " '% of Seated Capacity',\n",
       " '% of Total Capacity',\n",
       " 'Number of Readings']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the SWR passenger loadings data for P10 (the period used in the analysis)\n",
    "# The actual data is in the 'Loadings Data' sheet\n",
    "# The header has two rows: some columns span both rows, others have parent-child structure\n",
    "SWR_data = pd.read_excel(SWR_passenger_loadings_files[\"24-25 P10\"], sheet_name='Loadings Data', header=[0, 1])\n",
    "\n",
    "# Flatten the multi-level column names properly\n",
    "new_columns = []\n",
    "for col in SWR_data.columns:\n",
    "    # col is a tuple like ('Public Times', 'Arr') or ('Rail Period', nan)\n",
    "    if pd.notna(col[1]) and 'Unnamed' not in str(col[1]):\n",
    "        # Second row has a meaningful name (like Arr, Dep, or the last 4 columns)\n",
    "        new_columns.append(col[1])\n",
    "    elif pd.notna(col[0]) and 'Unnamed' not in str(col[0]):\n",
    "        # First row has the name\n",
    "        new_columns.append(col[0])\n",
    "    else:\n",
    "        # Both are unnamed, keep as is for now\n",
    "        new_columns.append(f\"{col[0]}_{col[1]}\")\n",
    "\n",
    "SWR_data.columns = new_columns\n",
    "\n",
    "SWR_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a3e8486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique locations: 199\n",
      "\n",
      "Unique locations: ['ADLESTN', 'ALDRSHT', 'ALTON', 'ANDOVER', 'ASCOT', 'ASFDMSX', 'ASHD', 'ASHH', 'ASHVALE', 'AXMNSTR', 'BAGSHOT', 'BARNES', 'BDHMPTN', 'BITERNE', 'BKNHRST', 'BLIEURD', 'BNSBDGE', 'BNTEY', 'BNTFORD', 'BOMO', 'BOOKHAM', 'BOTLEY', 'BOXHAWH', 'BRACKNL', 'BRANKSM', 'BRKWOOD', 'BRLANDS', 'BRUTON', 'BSNGSTK', 'BURSLDN', 'BYFLANH', 'CBHMSDA', 'CBRK', 'CCARY', 'CHISWCK', 'CHRISTC', 'CHSSN', 'CHSSS', 'CHTSEY', 'CLANDON', 'CLPHMJM', 'CLPHMJW', 'CLYGATE', 'CMBLEY', 'COSHAM', 'CRKRN', 'CSFD', 'DATCHET', 'DEAN', 'DORKING', 'DRCHS', 'EFNGHMJ', 'EGHAM', 'ELGH', 'EPSM', 'ERLFLD', 'ERLY', 'ESHER', 'EWELW', 'EXETERC', 'EXETRSD', 'FAREHAM', 'FARNHAM', 'FELTHAM', 'FENITON', 'FLEET', 'FRATTON', 'FRBRMN', 'FRIMLEY', 'FRNCMB', 'FROME', 'FULWELL', 'GDLMING', 'GLHM', 'GRATELY', 'GUILDFD', 'HAMPTON', 'HAMWICK', 'HASLEMR', 'HAVANT', 'HCRT', 'HEDGEND', 'HERSHAM', 'HILSEA', 'HINTONA', 'HMBLE', 'HMWTHY', 'HNCHLYW', 'HOLTONH', 'HONITON', 'HOOK', 'HOUNSLW', 'HRSLEY', 'ISLEWTH', 'KEWBDGE', 'KGSTON', 'KMPTNPK', 'LETHRHD', 'LIPHOOK', 'LISS', 'LMTNPIR', 'LMTNTWN', 'LNGCROS', 'LRDGFD', 'LYNDHRD', 'MALDENM', 'MBRK', 'MCHLDVR', 'MHERON', 'MLFORD', 'MORETON', 'MOTFONT', 'MOTSPRP', 'MRTLKE', 'NETLEY', 'NEWMLDN', 'NMILTON', 'NRBITON', 'NSHEEN', 'OVTN', 'OXSHOTT', 'PCHESTR', 'PHBR', 'PINHOE', 'POKSDWN', 'POOLE', 'PSEA', 'PSTONE', 'PTRSFLD', 'PUTNEY', 'QTRDBAT', 'RAYNSPK', 'RDNG4AB', 'RDNGSTN', 'REDBDGE', 'RICHMND', 'ROMSEY', 'RWLNDSC', 'SHAWFD', 'SHEPRTN', 'SHERBRN', 'SHOLING', 'SLSBRY', 'SOTON', 'SOTPKWY', 'STAINES', 'STDENYS', 'STLEIGH', 'STMGTS', 'STRWBYH', 'SUNBURY', 'SUNNGDL', 'SUNYMDS', 'SURBITN', 'SWAY', 'SWNWICK', 'SWYTHLN', 'SYONLA', 'TDITTON', 'TEDNGTN', 'TISBURY', 'TMPCMB', 'TOLWTH', 'TOTTON', 'TWCKNHM', 'UHALIFD', 'UPWEY', 'VAUXHLM', 'VRGNWTR', 'WANBRO', 'WARHAM', 'WATRLMN', 'WBYFLET', 'WDON', 'WDWTOWN', 'WEYBDGB', 'WEYBDGE', 'WEYMTH', 'WHIMPLE', 'WHTCHRH', 'WHTTON', 'WINERSH', 'WINETGL', 'WITLEY', 'WNCFILD', 'WNCHSTR', 'WOKING', 'WOKNGHM', 'WOLSTON', 'WONT', 'WOOL', 'WRCSTRP', 'WRMNSTR', 'WRPLSDN', 'WRYSBRY', 'WSORAER', 'WSTBRYW', 'YOVILJN', 'YOVILPM']\n"
     ]
    }
   ],
   "source": [
    "# Get unique locations from the data\n",
    "SWR_locations = sorted(SWR_data['Location'].unique())\n",
    "\n",
    "print(f\"Number of unique locations: {len(SWR_locations)}\")\n",
    "print(\"\\nUnique locations:\", SWR_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac98598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference data...\n",
      "Loaded 6104 TIPLOC to STANOX mappings\n",
      "\n",
      "Locations with STANOX codes:\n",
      "      TIPLOC STANOX\n",
      "0    ADLESTN  86011\n",
      "1    ALDRSHT  87031\n",
      "2      ALTON  87021\n",
      "3    ANDOVER  86074\n",
      "4      ASCOT  87103\n",
      "..       ...    ...\n",
      "194  WRYSBRY  87108\n",
      "195  WSORAER  87111\n",
      "196  WSTBRYW  82101\n",
      "197  YOVILJN  82341\n",
      "198  YOVILPM  82311\n",
      "\n",
      "[199 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import reference files and load TIPLOC to STANOX mapping\n",
    "import json\n",
    "from data.reference import reference_files\n",
    "\n",
    "# Load the reference data that contains TIPLOC to STANOX mapping\n",
    "print(\"Loading reference data...\")\n",
    "with open(reference_files[\"all dft categories\"], 'r') as f:\n",
    "    reference_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "stanox_ref = pd.DataFrame(reference_data)\n",
    "\n",
    "# Create TIPLOC to STANOX mapping dictionary\n",
    "# Check which column names are used in the reference data\n",
    "if 'tiploc' in stanox_ref.columns and 'stanox' in stanox_ref.columns:\n",
    "    tiploc_to_stanox = dict(zip(stanox_ref['tiploc'], stanox_ref['stanox']))\n",
    "elif 'tiploc_code' in stanox_ref.columns and 'stanox' in stanox_ref.columns:\n",
    "    tiploc_to_stanox = dict(zip(stanox_ref['tiploc_code'], stanox_ref['stanox']))\n",
    "else:\n",
    "    print(f\"Warning: Expected TIPLOC columns not found. Available columns: {list(stanox_ref.columns)}\")\n",
    "    tiploc_to_stanox = {}\n",
    "\n",
    "print(f\"Loaded {len(tiploc_to_stanox)} TIPLOC to STANOX mappings\")\n",
    "\n",
    "# Map STANOX codes to SWR locations\n",
    "SWR_stanox = [tiploc_to_stanox.get(loc, None) for loc in SWR_locations]\n",
    "\n",
    "# Create a DataFrame showing the mapping\n",
    "location_mapping = pd.DataFrame({\n",
    "    'TIPLOC': SWR_locations,\n",
    "    'STANOX': SWR_stanox\n",
    "})\n",
    "\n",
    "print(f\"\\nLocations with STANOX codes:\")\n",
    "print(location_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a0cc2",
   "metadata": {},
   "source": [
    "how many processed stanox do i have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae7924c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SWR locations: 199\n",
      "Total folders in processed_data: 375\n",
      "SWR locations with data in processed_data: 72\n",
      "SWR locations missing from processed_data: 125\n",
      "\n",
      "72 SWR stations with processed data:\n",
      "      TIPLOC STANOX\n",
      "1    ALDRSHT  87031\n",
      "2      ALTON  87021\n",
      "3    ANDOVER  86074\n",
      "4      ASCOT  87103\n",
      "5    ASFDMSX  87117\n",
      "11    BARNES  87149\n",
      "14   BKNHRST  86901\n",
      "18   BNTFORD  87144\n",
      "19      BOMO  86921\n",
      "23   BRACKNL  87101\n",
      "25   BRKWOOD  86041\n",
      "28   BSNGSTK  86066\n",
      "39   CLANDON  87069\n",
      "49   DORKING  87687\n",
      "52     EGHAM  87107\n",
      "53      ELGH  86087\n",
      "54      EPSM  87681\n",
      "58     EWELW  87304\n",
      "59   EXETERC  83431\n",
      "60   EXETRSD  83421\n",
      "61   FAREHAM  86241\n",
      "62   FARNHAM  87026\n",
      "63   FELTHAM  87121\n",
      "65     FLEET  86045\n",
      "67    FRBRMN  86042\n",
      "72   GDLMING  87057\n",
      "73      GLHM  82332\n",
      "75   GUILDFD  87052\n",
      "76   HAMPTON  87158\n",
      "78   HASLEMR  87062\n",
      "79    HAVANT  86341\n",
      "80      HCRT  87181\n",
      "82   HERSHAM  86003\n",
      "97   LETHRHD  87685\n",
      "109   MLFORD  87059\n",
      "112  MOTSPRP  87301\n",
      "113   MRTLKE  87139\n",
      "115  NEWMLDN  87169\n",
      "116  NMILTON  86911\n",
      "117  NRBITON  87163\n",
      "122     PHBR  86311\n",
      "125    POOLE  86935\n",
      "126     PSEA  86313\n",
      "128  PTRSFLD  87066\n",
      "129   PUTNEY  87152\n",
      "131  RAYNSPK  87271\n",
      "133  RDNGSTN  74237\n",
      "135  RICHMND  87135\n",
      "142   SLSBRY  86122\n",
      "143    SOTON  86520\n",
      "145  STAINES  87114\n",
      "147  STLEIGH  87303\n",
      "148   STMGTS  87133\n",
      "149  STRWBYH  87134\n",
      "151  SUNNGDL  87104\n",
      "153  SURBITN  87171\n",
      "159  TEDNGTN  87159\n",
      "164  TWCKNHM  87131\n",
      "167  VAUXHLM  87214\n",
      "169   WANBRO  87016\n",
      "170   WARHAM  86964\n",
      "172  WBYFLET  86023\n",
      "173     WDON  87261\n",
      "174  WDWTOWN  87153\n",
      "175  WEYBDGB  86005\n",
      "176  WEYBDGE  86005\n",
      "177   WEYMTH  86981\n",
      "180   WHTTON  87126\n",
      "185  WNCHSTR  86083\n",
      "186   WOKING  86031\n",
      "187  WOKNGHM  87004\n",
      "191  WRCSTRP  87302\n"
     ]
    }
   ],
   "source": [
    "# Check which STANOX codes exist in the processed_data folder\n",
    "processed_data_dir = os.path.join(project_root, 'processed_data')\n",
    "\n",
    "# Get all folder names in processed_data (these should be STANOX codes)\n",
    "if os.path.exists(processed_data_dir):\n",
    "    existing_stanox_folders = [folder for folder in os.listdir(processed_data_dir) \n",
    "                               if os.path.isdir(os.path.join(processed_data_dir, folder))]\n",
    "    \n",
    "    # Convert to integers for comparison (STANOX codes are numeric)\n",
    "    existing_stanox_set = set(existing_stanox_folders)\n",
    "    \n",
    "    # Check which SWR STANOX codes exist in processed_data\n",
    "    # Remove None values from SWR_stanox\n",
    "    swr_stanox_str = [str(stanox) for stanox in SWR_stanox if stanox is not None]\n",
    "    \n",
    "    # Find matches\n",
    "    matching_stanox = [stanox for stanox in swr_stanox_str if stanox in existing_stanox_set]\n",
    "    missing_stanox = [stanox for stanox in swr_stanox_str if stanox not in existing_stanox_set]\n",
    "    \n",
    "    print(f\"Total SWR locations: {len(SWR_locations)}\")\n",
    "    print(f\"Total folders in processed_data: {len(existing_stanox_folders)}\")\n",
    "    print(f\"SWR locations with data in processed_data: {len(matching_stanox)}\")\n",
    "    print(f\"SWR locations missing from processed_data: {len(missing_stanox)}\")\n",
    "    \n",
    "    # Show which ones are available\n",
    "    print(f\"\\n{len(matching_stanox)} SWR stations with processed data:\")\n",
    "    \n",
    "    # Create a mapping to show TIPLOC names with their STANOX\n",
    "    SWR_available_stations = location_mapping[location_mapping['STANOX'].astype(str).isin(matching_stanox)]\n",
    "    print(SWR_available_stations.to_string())\n",
    "\n",
    "else:\n",
    "    print(f\"processed_data directory not found at: {processed_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382312a",
   "metadata": {},
   "source": [
    "\"SWR_available_stations\" stores the available stations pre-processed and in SWR routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51ac893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique O-D pairs in SWR data (with valid STANOX codes): 183\n",
      "\n",
      "Processing Summary:\n",
      "  Successfully processed files: 504\n",
      "  Corrupted/unreadable files: 0\n",
      "\n",
      "O-D Pair Analysis:\n",
      "  Total unique O-D pairs in SWR data: 183\n",
      "  Total O-D pairs checked in preprocessed data: 22101\n",
      "  Unique matching O-D pairs found in both datasets: 181\n",
      "  Total matches (including duplicates across stations): 8596\n",
      "\n",
      "Matching O-D pairs by station (top 20):\n",
      "   STANOX   TIPLOC  Num_Matching_OD_Pairs\n",
      "63  87261     WDON                    365\n",
      "8   86031   WOKING                    332\n",
      "60  87171  SURBITN                    323\n",
      "59  87169  NEWMLDN                    310\n",
      "62  87214  VAUXHLM                    269\n",
      "12  86066  BSNGSTK                    249\n",
      "53  87149   BARNES                    218\n",
      "47  87131  TWCKNHM                    216\n",
      "21  86520    SOTON                    214\n",
      "15  86087     ELGH                    210\n",
      "43  87114  STAINES                    205\n",
      "10  86042   FRBRMN                    194\n",
      "16  86122   SLSBRY                    190\n",
      "50  87135  RICHMND                    185\n",
      "33  87052  GUILDFD                    183\n",
      "54  87152   PUTNEY                    177\n",
      "64  87271  RAYNSPK                    175\n",
      "45  87121  FELTHAM                    166\n",
      "24  86921     BOMO                    158\n",
      "14  86083  WNCHSTR                    151\n",
      "\n",
      "Sample of matching O-D pairs (first 20):\n",
      "     TIPLOC Origin_TIPLOC Origin_STANOX Destination_TIPLOC Destination_STANOX\n",
      "0   ALDRSHT        WOKING         86031              ALTON              87021\n",
      "1   ALDRSHT       WATRLMN         87212              ALTON              87021\n",
      "2   ALDRSHT         ALTON         87021            WATRLMN              87212\n",
      "3   ALDRSHT         ALTON         87021             WOKING              86031\n",
      "4   ALDRSHT       FARNHAM         87026            WATRLMN              87212\n",
      "5   ALDRSHT       WATRLMN         87212            ALDRSHT              87031\n",
      "6   ALDRSHT       FARNHAM         87026            GUILDFD              87052\n",
      "7   ALDRSHT       GUILDFD         87052            FARNHAM              87026\n",
      "8   ALDRSHT         ASCOT         87103            ALDRSHT              87031\n",
      "9   ALDRSHT       ALDRSHT         87031              ASCOT              87103\n",
      "10  ALDRSHT         ASCOT         87103            FARNHAM              87026\n",
      "11  ALDRSHT        WOKING         86031              ALTON              87021\n",
      "12  ALDRSHT       WATRLMN         87212              ALTON              87021\n",
      "13  ALDRSHT         ALTON         87021            WATRLMN              87212\n",
      "14  ALDRSHT         ALTON         87021             WOKING              86031\n",
      "15  ALDRSHT       FARNHAM         87026            WATRLMN              87212\n",
      "16  ALDRSHT       WATRLMN         87212            ALDRSHT              87031\n",
      "17  ALDRSHT       FARNHAM         87026            GUILDFD              87052\n",
      "18  ALDRSHT       GUILDFD         87052            FARNHAM              87026\n",
      "19  ALDRSHT         ASCOT         87103            ALDRSHT              87031\n",
      "\n",
      "Processing Summary:\n",
      "  Successfully processed files: 504\n",
      "  Corrupted/unreadable files: 0\n",
      "\n",
      "O-D Pair Analysis:\n",
      "  Total unique O-D pairs in SWR data: 183\n",
      "  Total O-D pairs checked in preprocessed data: 22101\n",
      "  Unique matching O-D pairs found in both datasets: 181\n",
      "  Total matches (including duplicates across stations): 8596\n",
      "\n",
      "Matching O-D pairs by station (top 20):\n",
      "   STANOX   TIPLOC  Num_Matching_OD_Pairs\n",
      "63  87261     WDON                    365\n",
      "8   86031   WOKING                    332\n",
      "60  87171  SURBITN                    323\n",
      "59  87169  NEWMLDN                    310\n",
      "62  87214  VAUXHLM                    269\n",
      "12  86066  BSNGSTK                    249\n",
      "53  87149   BARNES                    218\n",
      "47  87131  TWCKNHM                    216\n",
      "21  86520    SOTON                    214\n",
      "15  86087     ELGH                    210\n",
      "43  87114  STAINES                    205\n",
      "10  86042   FRBRMN                    194\n",
      "16  86122   SLSBRY                    190\n",
      "50  87135  RICHMND                    185\n",
      "33  87052  GUILDFD                    183\n",
      "54  87152   PUTNEY                    177\n",
      "64  87271  RAYNSPK                    175\n",
      "45  87121  FELTHAM                    166\n",
      "24  86921     BOMO                    158\n",
      "14  86083  WNCHSTR                    151\n",
      "\n",
      "Sample of matching O-D pairs (first 20):\n",
      "     TIPLOC Origin_TIPLOC Origin_STANOX Destination_TIPLOC Destination_STANOX\n",
      "0   ALDRSHT        WOKING         86031              ALTON              87021\n",
      "1   ALDRSHT       WATRLMN         87212              ALTON              87021\n",
      "2   ALDRSHT         ALTON         87021            WATRLMN              87212\n",
      "3   ALDRSHT         ALTON         87021             WOKING              86031\n",
      "4   ALDRSHT       FARNHAM         87026            WATRLMN              87212\n",
      "5   ALDRSHT       WATRLMN         87212            ALDRSHT              87031\n",
      "6   ALDRSHT       FARNHAM         87026            GUILDFD              87052\n",
      "7   ALDRSHT       GUILDFD         87052            FARNHAM              87026\n",
      "8   ALDRSHT         ASCOT         87103            ALDRSHT              87031\n",
      "9   ALDRSHT       ALDRSHT         87031              ASCOT              87103\n",
      "10  ALDRSHT         ASCOT         87103            FARNHAM              87026\n",
      "11  ALDRSHT        WOKING         86031              ALTON              87021\n",
      "12  ALDRSHT       WATRLMN         87212              ALTON              87021\n",
      "13  ALDRSHT         ALTON         87021            WATRLMN              87212\n",
      "14  ALDRSHT         ALTON         87021             WOKING              86031\n",
      "15  ALDRSHT       FARNHAM         87026            WATRLMN              87212\n",
      "16  ALDRSHT       WATRLMN         87212            ALDRSHT              87031\n",
      "17  ALDRSHT       FARNHAM         87026            GUILDFD              87052\n",
      "18  ALDRSHT       GUILDFD         87052            FARNHAM              87026\n",
      "19  ALDRSHT         ASCOT         87103            ALDRSHT              87031\n"
     ]
    }
   ],
   "source": [
    "# Check O-D pairs present in both SWR data and preprocessed data\n",
    "# First, convert SWR TIPLOC codes to STANOX codes for comparison\n",
    "\n",
    "# Add STANOX codes to SWR data for Origin and Destination\n",
    "SWR_data['Origin_STANOX'] = SWR_data['Origin'].map(tiploc_to_stanox)\n",
    "SWR_data['Destination_STANOX'] = SWR_data['Destination'].map(tiploc_to_stanox)\n",
    "\n",
    "# Get unique O-D pairs from SWR data (using STANOX codes)\n",
    "swr_od_pairs_stanox = SWR_data[['Origin', 'Origin_STANOX', 'Destination', 'Destination_STANOX']].dropna().drop_duplicates()\n",
    "print(f\"Total unique O-D pairs in SWR data (with valid STANOX codes): {len(swr_od_pairs_stanox)}\")\n",
    "\n",
    "# Initialize a list to store matching O-D pairs\n",
    "matching_od_pairs = []\n",
    "total_processed_od_pairs = 0\n",
    "corrupted_files = []\n",
    "processed_files = 0\n",
    "\n",
    "# For each available SWR station, check its preprocessed data\n",
    "for idx, row in SWR_available_stations.iterrows():\n",
    "    stanox = str(row['STANOX'])\n",
    "    tiploc = row['TIPLOC']\n",
    "    \n",
    "    # Path to the preprocessed data for this station\n",
    "    station_dir = os.path.join(processed_data_dir, stanox)\n",
    "    \n",
    "    # Look for parquet files in the station directory\n",
    "    if os.path.exists(station_dir):\n",
    "        parquet_files = [f for f in os.listdir(station_dir) if f.endswith('.parquet')]\n",
    "        \n",
    "        for pq_file in parquet_files:\n",
    "            try:\n",
    "                # Read the preprocessed data using fastparquet engine\n",
    "                preprocessed_data = pd.read_parquet(os.path.join(station_dir, pq_file), engine='fastparquet')\n",
    "                processed_files += 1\n",
    "                \n",
    "                # Check if the required columns exist\n",
    "                if 'PLANNED_ORIGIN_LOCATION_CODE' in preprocessed_data.columns and \\\n",
    "                   'PLANNED_DEST_LOCATION_CODE' in preprocessed_data.columns:\n",
    "                    \n",
    "                    # Get unique O-D pairs from preprocessed data (already in STANOX format)\n",
    "                    processed_od = preprocessed_data[['PLANNED_ORIGIN_LOCATION_CODE', \n",
    "                                                       'PLANNED_DEST_LOCATION_CODE']].drop_duplicates()\n",
    "                    total_processed_od_pairs += len(processed_od)\n",
    "                    \n",
    "                    # Check for matches with SWR O-D pairs (comparing STANOX to STANOX)\n",
    "                    for _, swr_row in swr_od_pairs_stanox.iterrows():\n",
    "                        swr_origin_tiploc = swr_row['Origin']\n",
    "                        swr_origin_stanox = swr_row['Origin_STANOX']\n",
    "                        swr_dest_tiploc = swr_row['Destination']\n",
    "                        swr_dest_stanox = swr_row['Destination_STANOX']\n",
    "                        \n",
    "                        # Check if this O-D pair exists in the preprocessed data\n",
    "                        match = processed_od[\n",
    "                            (processed_od['PLANNED_ORIGIN_LOCATION_CODE'] == swr_origin_stanox) &\n",
    "                            (processed_od['PLANNED_DEST_LOCATION_CODE'] == swr_dest_stanox)\n",
    "                        ]\n",
    "                        \n",
    "                        if not match.empty:\n",
    "                            matching_od_pairs.append({\n",
    "                                'STANOX': stanox,\n",
    "                                'TIPLOC': tiploc,\n",
    "                                'Origin_TIPLOC': swr_origin_tiploc,\n",
    "                                'Origin_STANOX': swr_origin_stanox,\n",
    "                                'Destination_TIPLOC': swr_dest_tiploc,\n",
    "                                'Destination_STANOX': swr_dest_stanox,\n",
    "                                'File': pq_file\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                # Track corrupted files but continue processing\n",
    "                corrupted_files.append({'STANOX': stanox, 'File': pq_file, 'Error': str(e)})\n",
    "\n",
    "# Create DataFrame of matching O-D pairs\n",
    "matching_od_df = pd.DataFrame(matching_od_pairs)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nProcessing Summary:\")\n",
    "print(f\"  Successfully processed files: {processed_files}\")\n",
    "print(f\"  Corrupted/unreadable files: {len(corrupted_files)}\")\n",
    "\n",
    "# Get unique O-D pairs (might be same O-D pair in multiple stations)\n",
    "if len(matching_od_df) > 0:\n",
    "    unique_matching_od = matching_od_df[['Origin_STANOX', 'Destination_STANOX']].drop_duplicates()\n",
    "    \n",
    "    print(f\"\\nO-D Pair Analysis:\")\n",
    "    print(f\"  Total unique O-D pairs in SWR data: {len(swr_od_pairs_stanox)}\")\n",
    "    print(f\"  Total O-D pairs checked in preprocessed data: {total_processed_od_pairs}\")\n",
    "    print(f\"  Unique matching O-D pairs found in both datasets: {len(unique_matching_od)}\")\n",
    "    print(f\"  Total matches (including duplicates across stations): {len(matching_od_df)}\")\n",
    "    \n",
    "    # Show summary by station\n",
    "    station_summary = matching_od_df.groupby(['STANOX', 'TIPLOC']).size().reset_index(name='Num_Matching_OD_Pairs')\n",
    "    print(f\"\\nMatching O-D pairs by station (top 20):\")\n",
    "    print(station_summary.nlargest(20, 'Num_Matching_OD_Pairs').to_string())\n",
    "    \n",
    "    print(f\"\\nSample of matching O-D pairs (first 20):\")\n",
    "    print(matching_od_df[['TIPLOC', 'Origin_TIPLOC', 'Origin_STANOX', 'Destination_TIPLOC', 'Destination_STANOX']].head(20).to_string())\n",
    "else:\n",
    "    print(\"\\nNo matching O-D pairs found between SWR and preprocessed data\")\n",
    "    print(\"This might mean the O-D pairs in SWR data don't match any routes in the preprocessed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b84851c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking EVENT_DATETIME availability (with STANOX from folder name):\n",
      "======================================================================\n",
      "\n",
      "Total unique matched O-D pairs: 181\n",
      "\n",
      "Results:\n",
      "  Stations with matched O-D data: 504\n",
      "  Total records with matched O-D pairs: 1412645\n",
      "  Records WITH EVENT_DATETIME (had delays): 284286 (20.1%)\n",
      "  Records WITHOUT EVENT_DATETIME (no delays): 1128359 (79.9%)\n",
      "\n",
      "✓ Good news! 284286 train records have delay information (EVENT_DATETIME).\n",
      "  We can use these for date-based matching with SWR data.\n",
      "\n",
      "  The remaining 1128359 records without delays can still be matched\n",
      "  using WEEKDAY for day-of-week patterns.\n",
      "\n",
      "Results:\n",
      "  Stations with matched O-D data: 504\n",
      "  Total records with matched O-D pairs: 1412645\n",
      "  Records WITH EVENT_DATETIME (had delays): 284286 (20.1%)\n",
      "  Records WITHOUT EVENT_DATETIME (no delays): 1128359 (79.9%)\n",
      "\n",
      "✓ Good news! 284286 train records have delay information (EVENT_DATETIME).\n",
      "  We can use these for date-based matching with SWR data.\n",
      "\n",
      "  The remaining 1128359 records without delays can still be matched\n",
      "  using WEEKDAY for day-of-week patterns.\n"
     ]
    }
   ],
   "source": [
    "# Check EVENT_DATETIME availability - properly adding STANOX from folder name\n",
    "print(\"Checking EVENT_DATETIME availability (with STANOX from folder name):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the matched O-D pairs from earlier analysis\n",
    "matched_od_stanox_pairs = set(zip(matching_od_df['Origin_STANOX'], matching_od_df['Destination_STANOX']))\n",
    "\n",
    "print(f\"\\nTotal unique matched O-D pairs: {len(matched_od_stanox_pairs)}\")\n",
    "\n",
    "# Statistics across all available stations\n",
    "total_records = 0\n",
    "records_with_datetime = 0\n",
    "records_without_datetime = 0\n",
    "stations_with_data = 0\n",
    "\n",
    "# Check each available SWR station\n",
    "for idx, row in SWR_available_stations.iterrows():\n",
    "    stanox = str(row['STANOX'])\n",
    "    tiploc = row['TIPLOC']\n",
    "    \n",
    "    station_dir = os.path.join(processed_data_dir, stanox)\n",
    "    \n",
    "    if os.path.exists(station_dir):\n",
    "        parquet_files = [f for f in os.listdir(station_dir) if f.endswith('.parquet')]\n",
    "        \n",
    "        for pq_file in parquet_files:\n",
    "            try:\n",
    "                # Read the preprocessed data\n",
    "                preprocessed_data = pd.read_parquet(os.path.join(station_dir, pq_file), engine='fastparquet')\n",
    "                \n",
    "                # ADD STANOX COLUMN FROM FOLDER NAME\n",
    "                preprocessed_data['STANOX'] = int(stanox)\n",
    "                \n",
    "                # Check if required columns exist\n",
    "                if all(col in preprocessed_data.columns for col in \n",
    "                       ['PLANNED_ORIGIN_LOCATION_CODE', 'PLANNED_DEST_LOCATION_CODE', 'EVENT_DATETIME']):\n",
    "                    \n",
    "                    # Filter to only matched O-D pairs\n",
    "                    preprocessed_data['OD_pair'] = list(zip(\n",
    "                        preprocessed_data['PLANNED_ORIGIN_LOCATION_CODE'], \n",
    "                        preprocessed_data['PLANNED_DEST_LOCATION_CODE']\n",
    "                    ))\n",
    "                    \n",
    "                    matched_records = preprocessed_data[preprocessed_data['OD_pair'].isin(matched_od_stanox_pairs)]\n",
    "                    \n",
    "                    if len(matched_records) > 0:\n",
    "                        stations_with_data += 1\n",
    "                        total_records += len(matched_records)\n",
    "                        \n",
    "                        # Check which have EVENT_DATETIME\n",
    "                        has_datetime = matched_records['EVENT_DATETIME'].notna()\n",
    "                        records_with_datetime += has_datetime.sum()\n",
    "                        records_without_datetime += (~has_datetime).sum()\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Stations with matched O-D data: {stations_with_data}\")\n",
    "print(f\"  Total records with matched O-D pairs: {total_records}\")\n",
    "\n",
    "if total_records > 0:\n",
    "    print(f\"  Records WITH EVENT_DATETIME (had delays): {records_with_datetime} ({records_with_datetime/total_records*100:.1f}%)\")\n",
    "    print(f\"  Records WITHOUT EVENT_DATETIME (no delays): {records_without_datetime} ({records_without_datetime/total_records*100:.1f}%)\")\n",
    "    \n",
    "    if records_with_datetime > 0:\n",
    "        print(f\"\\n✓ Good news! {records_with_datetime} train records have delay information (EVENT_DATETIME).\")\n",
    "        print(f\"  We can use these for date-based matching with SWR data.\")\n",
    "        print(f\"\\n  The remaining {records_without_datetime} records without delays can still be matched\")\n",
    "        print(f\"  using WEEKDAY for day-of-week patterns.\")\n",
    "    else:\n",
    "        print(f\"\\n✗ All {records_without_datetime} matched train records have no delays (EVENT_DATETIME is None).\")\n",
    "        print(f\"  We should use WEEKDAY column for day-of-week matching instead.\")\n",
    "else:\n",
    "    print(f\"\\n✗ No records found with the matched O-D pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c05ddfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking date range of preprocessed data:\n",
      "======================================================================\n",
      "SWR data period: 2024-12-08 to 2025-01-04\n",
      "Checking preprocessed data dates...\n",
      "\n",
      "\n",
      "No valid dates found in the sample. Most trains may not have had delays.\n"
     ]
    }
   ],
   "source": [
    "# Check the date range of preprocessed data (using correct format: DD-MMM-YYYY)\n",
    "print(\"Checking date range of preprocessed data:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "swr_start = pd.to_datetime('2024-12-08')\n",
    "swr_end = pd.to_datetime('2025-01-04')\n",
    "\n",
    "print(f\"SWR data period: {swr_start.date()} to {swr_end.date()}\")\n",
    "print(f\"Checking preprocessed data dates...\\n\")\n",
    "\n",
    "all_dates = []\n",
    "files_checked = 0\n",
    "\n",
    "# Sample some files to check date range\n",
    "for idx, row in SWR_available_stations.head(10).iterrows():\n",
    "    stanox = str(row['STANOX'])\n",
    "    tiploc = row['TIPLOC']\n",
    "    \n",
    "    station_dir = os.path.join(processed_data_dir, stanox)\n",
    "    \n",
    "    if os.path.exists(station_dir):\n",
    "        parquet_files = [f for f in os.listdir(station_dir) if f.endswith('.parquet')]\n",
    "        \n",
    "        for pq_file in parquet_files[:2]:  # Check first 2 files per station\n",
    "            try:\n",
    "                preprocessed_data = pd.read_parquet(os.path.join(station_dir, pq_file), engine='fastparquet')\n",
    "                \n",
    "                if 'EVENT_DATETIME' in preprocessed_data.columns:\n",
    "                    # Parse dates with correct format: DD-MMM-YYYY (e.g., \"13-JUN-2024\")\n",
    "                    dates = pd.to_datetime(preprocessed_data['EVENT_DATETIME'], format='%d-%b-%Y', errors='coerce')\n",
    "                    valid_dates = dates.dropna()\n",
    "                    \n",
    "                    if len(valid_dates) > 0:\n",
    "                        all_dates.extend(valid_dates.tolist())\n",
    "                        files_checked += 1\n",
    "                        \n",
    "                        if files_checked <= 3:  # Show details for first few files\n",
    "                            print(f\"{tiploc} ({pq_file}):\")\n",
    "                            print(f\"  Date range: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "                            print(f\"  Sample dates: {[d.strftime('%d-%b-%Y') for d in valid_dates.head(3)]}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "if all_dates:\n",
    "    all_dates_series = pd.Series(all_dates)\n",
    "    overall_min = all_dates_series.min()\n",
    "    overall_max = all_dates_series.max()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Overall preprocessed data date range (from {files_checked} files):\")\n",
    "    print(f\"  Earliest date: {overall_min.date()}\")\n",
    "    print(f\"  Latest date: {overall_max.date()}\")\n",
    "    print(f\"  Total span: {(overall_max - overall_min).days} days\")\n",
    "    \n",
    "    # Check if SWR period is in the data\n",
    "    dates_in_swr_period = all_dates_series[(all_dates_series >= swr_start) & (all_dates_series <= swr_end)]\n",
    "    \n",
    "    print(f\"\\nDates in SWR period (Dec 8, 2024 - Jan 4, 2025):\")\n",
    "    print(f\"  Found: {len(dates_in_swr_period)} records\")\n",
    "    \n",
    "    if len(dates_in_swr_period) > 0:\n",
    "        print(f\"  ✓ The SWR period IS covered in the preprocessed data!\")\n",
    "        print(f\"  Date range in SWR period: {dates_in_swr_period.min().date()} to {dates_in_swr_period.max().date()}\")\n",
    "    else:\n",
    "        print(f\"  ✗ The SWR period is NOT in the preprocessed data.\")\n",
    "        print(f\"  The data covers: {overall_min.date()} to {overall_max.date()}\")\n",
    "        print(f\"\\n  Since dates don't overlap, we'll use DAY-OF-WEEK matching instead.\")\n",
    "else:\n",
    "    print(\"\\nNo valid dates found in the sample. Most trains may not have had delays.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79854090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding date range from files with delay data:\n",
      "======================================================================\n",
      "Files checked: 504\n",
      "Files with valid dates: 0\n",
      "Total date records found: 0\n",
      "\n",
      "No EVENT_DATETIME values found. Will use WEEKDAY column for day-of-week matching.\n"
     ]
    }
   ],
   "source": [
    "# More thorough check - look for files that have EVENT_DATETIME values\n",
    "print(\"Finding date range from files with delay data:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_dates_found = []\n",
    "files_with_dates = 0\n",
    "total_files_checked = 0\n",
    "\n",
    "# Check all available SWR stations\n",
    "for idx, row in SWR_available_stations.iterrows():\n",
    "    stanox = str(row['STANOX'])\n",
    "    tiploc = row['TIPLOC']\n",
    "    \n",
    "    station_dir = os.path.join(processed_data_dir, stanox)\n",
    "    \n",
    "    if os.path.exists(station_dir):\n",
    "        parquet_files = [f for f in os.listdir(station_dir) if f.endswith('.parquet')]\n",
    "        \n",
    "        for pq_file in parquet_files:\n",
    "            try:\n",
    "                preprocessed_data = pd.read_parquet(os.path.join(station_dir, pq_file), engine='fastparquet')\n",
    "                total_files_checked += 1\n",
    "                \n",
    "                if 'EVENT_DATETIME' in preprocessed_data.columns:\n",
    "                    # Parse dates with format DD-MMM-YYYY\n",
    "                    dates = pd.to_datetime(preprocessed_data['EVENT_DATETIME'], format='%d-%b-%Y', errors='coerce')\n",
    "                    valid_dates = dates.dropna()\n",
    "                    \n",
    "                    if len(valid_dates) > 0:\n",
    "                        all_dates_found.extend(valid_dates.tolist())\n",
    "                        files_with_dates += 1\n",
    "                        \n",
    "                        # Stop after finding enough samples\n",
    "                        if files_with_dates >= 20:\n",
    "                            break\n",
    "                            \n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    if files_with_dates >= 20:\n",
    "        break\n",
    "\n",
    "print(f\"Files checked: {total_files_checked}\")\n",
    "print(f\"Files with valid dates: {files_with_dates}\")\n",
    "print(f\"Total date records found: {len(all_dates_found)}\")\n",
    "\n",
    "if all_dates_found:\n",
    "    dates_series = pd.Series(all_dates_found)\n",
    "    \n",
    "    print(f\"\\nDate range in preprocessed data:\")\n",
    "    print(f\"  Earliest: {dates_series.min().date()} ({dates_series.min().strftime('%A')})\")\n",
    "    print(f\"  Latest: {dates_series.max().date()} ({dates_series.max().strftime('%A')})\")\n",
    "    print(f\"  Span: {(dates_series.max() - dates_series.min()).days} days\")\n",
    "    \n",
    "    # Check overlap with SWR period\n",
    "    swr_start = pd.to_datetime('2024-12-08')\n",
    "    swr_end = pd.to_datetime('2025-01-04')\n",
    "    \n",
    "    in_swr_period = dates_series[(dates_series >= swr_start) & (dates_series <= swr_end)]\n",
    "    \n",
    "    print(f\"\\nSWR period: {swr_start.date()} to {swr_end.date()}\")\n",
    "    print(f\"Records in SWR period: {len(in_swr_period)}\")\n",
    "    \n",
    "    if len(in_swr_period) > 0:\n",
    "        print(f\"✓ SWR period IS covered! Can use exact date matching.\")\n",
    "        print(f\"  Dates found: {in_swr_period.min().date()} to {in_swr_period.max().date()}\")\n",
    "    else:\n",
    "        print(f\"✗ SWR period NOT covered in preprocessed data.\")\n",
    "        print(f\"  Will use WEEKDAY-based matching instead.\")\n",
    "        \n",
    "    # Show date distribution by month\n",
    "    print(f\"\\nDate distribution by month:\")\n",
    "    month_counts = dates_series.dt.to_period('M').value_counts().sort_index()\n",
    "    for period, count in month_counts.head(10).items():\n",
    "        print(f\"  {period}: {count} records\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nNo EVENT_DATETIME values found. Will use WEEKDAY column for day-of-week matching.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0560ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking actual EVENT_DATETIME format:\n",
      "======================================================================\n",
      "Found non-null EVENT_DATETIME in: ALDRSHT/FR.parquet\n",
      "Number of non-null values: 899\n",
      "Data type: object\n",
      "\n",
      "Sample values:\n",
      "  [0] 30-MAR-2024 00:03 (type: <class 'str'>)\n",
      "  [1] 22-JUN-2024 00:09 (type: <class 'str'>)\n",
      "  [2] 10-AUG-2024 00:15 (type: <class 'str'>)\n",
      "  [3] 03-AUG-2024 00:19 (type: <class 'str'>)\n",
      "  [4] 10-FEB-2024 00:22 (type: <class 'str'>)\n",
      "  [5] 13-APR-2024 00:22 (type: <class 'str'>)\n",
      "  [6] 10-AUG-2024 00:22 (type: <class 'str'>)\n",
      "  [7] 20-APR-2024 00:25 (type: <class 'str'>)\n",
      "  [8] 10-FEB-2024 00:26 (type: <class 'str'>)\n",
      "  [9] 21-SEP-2024 00:26 (type: <class 'str'>)\n",
      "\n",
      "First value details:\n",
      "  Value: 30-MAR-2024 00:03\n",
      "  Type: <class 'str'>\n",
      "  String representation: '30-MAR-2024 00:03'\n"
     ]
    }
   ],
   "source": [
    "# Check the actual format of EVENT_DATETIME values\n",
    "print(\"Checking actual EVENT_DATETIME format:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "found_sample = False\n",
    "\n",
    "for idx, row in SWR_available_stations.iterrows():\n",
    "    if found_sample:\n",
    "        break\n",
    "        \n",
    "    stanox = str(row['STANOX'])\n",
    "    tiploc = row['TIPLOC']\n",
    "    \n",
    "    station_dir = os.path.join(processed_data_dir, stanox)\n",
    "    \n",
    "    if os.path.exists(station_dir):\n",
    "        parquet_files = [f for f in os.listdir(station_dir) if f.endswith('.parquet')]\n",
    "        \n",
    "        for pq_file in parquet_files:\n",
    "            try:\n",
    "                preprocessed_data = pd.read_parquet(os.path.join(station_dir, pq_file), engine='fastparquet')\n",
    "                \n",
    "                if 'EVENT_DATETIME' in preprocessed_data.columns:\n",
    "                    # Get non-null EVENT_DATETIME values\n",
    "                    non_null_dates = preprocessed_data['EVENT_DATETIME'].dropna()\n",
    "                    \n",
    "                    if len(non_null_dates) > 0:\n",
    "                        print(f\"Found non-null EVENT_DATETIME in: {tiploc}/{pq_file}\")\n",
    "                        print(f\"Number of non-null values: {len(non_null_dates)}\")\n",
    "                        print(f\"Data type: {non_null_dates.dtype}\")\n",
    "                        print(f\"\\nSample values:\")\n",
    "                        for i, val in enumerate(non_null_dates.head(10)):\n",
    "                            print(f\"  [{i}] {val} (type: {type(val)})\")\n",
    "                        \n",
    "                        # Try to understand the format\n",
    "                        sample_val = non_null_dates.iloc[0]\n",
    "                        print(f\"\\nFirst value details:\")\n",
    "                        print(f\"  Value: {sample_val}\")\n",
    "                        print(f\"  Type: {type(sample_val)}\")\n",
    "                        print(f\"  String representation: '{str(sample_val)}'\")\n",
    "                        \n",
    "                        found_sample = True\n",
    "                        break\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {pq_file}: {e}\")\n",
    "\n",
    "if not found_sample:\n",
    "    print(\"No non-null EVENT_DATETIME values found in any file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aeed90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking preprocessed data date range (correct format with time):\n",
      "======================================================================\n",
      "Files with valid dates: 50\n",
      "Total delay records found: 32380\n",
      "\n",
      "Preprocessed data date range:\n",
      "  Earliest: 04-Feb-2024 01:15 (Sunday)\n",
      "  Latest: 05-Jan-2025 00:33 (Sunday)\n",
      "  Total span: 335 days\n",
      "\n",
      "SWR period: 08-Dec-2024 to 04-Jan-2025\n",
      "Delay records in SWR period: 2849\n",
      "✓ SWR period IS covered in the data!\n",
      "  Range: 08-Dec-2024 to 03-Jan-2025\n",
      "\n",
      "  → Can use EXACT DATE matching for trains with delays in SWR period\n",
      "\n",
      "  → Will use WEEKDAY matching for all trains (including those without delays)\n",
      "\n",
      "Monthly distribution of delay records:\n",
      "  2024-02: 3,485 records\n",
      "  2024-03: 3,088 records\n",
      "  2024-04: 1,606 records\n",
      "  2024-05: 1,780 records\n",
      "  2024-06: 2,380 records\n",
      "  2024-07: 2,572 records\n",
      "  2024-08: 2,176 records\n",
      "  2024-09: 2,777 records\n",
      "  2024-10: 3,387 records\n",
      "  2024-11: 4,727 records\n",
      "  2024-12: 3,957 records\n",
      "  2025-01: 445 records\n"
     ]
    }
   ],
   "source": [
    "# Check date range with correct format: DD-MMM-YYYY HH:MM\n",
    "print(\"Checking preprocessed data date range (correct format with time):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_dates_correct = []\n",
    "files_with_dates_correct = 0\n",
    "\n",
    "for idx, row in SWR_available_stations.iterrows():\n",
    "    stanox = str(row['STANOX'])\n",
    "    station_dir = os.path.join(processed_data_dir, stanox)\n",
    "    \n",
    "    if os.path.exists(station_dir):\n",
    "        parquet_files = [f for f in os.listdir(station_dir) if f.endswith('.parquet')]\n",
    "        \n",
    "        for pq_file in parquet_files:\n",
    "            try:\n",
    "                preprocessed_data = pd.read_parquet(os.path.join(station_dir, pq_file), engine='fastparquet')\n",
    "                \n",
    "                if 'EVENT_DATETIME' in preprocessed_data.columns:\n",
    "                    # Parse with correct format: DD-MMM-YYYY HH:MM\n",
    "                    dates = pd.to_datetime(preprocessed_data['EVENT_DATETIME'], format='%d-%b-%Y %H:%M', errors='coerce')\n",
    "                    valid_dates = dates.dropna()\n",
    "                    \n",
    "                    if len(valid_dates) > 0:\n",
    "                        all_dates_correct.extend(valid_dates.tolist())\n",
    "                        files_with_dates_correct += 1\n",
    "                        \n",
    "                        if files_with_dates_correct >= 50:  # Sample 50 files\n",
    "                            break\n",
    "                            \n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    if files_with_dates_correct >= 50:\n",
    "        break\n",
    "\n",
    "print(f\"Files with valid dates: {files_with_dates_correct}\")\n",
    "print(f\"Total delay records found: {len(all_dates_correct)}\")\n",
    "\n",
    "if all_dates_correct:\n",
    "    dates_series = pd.Series(all_dates_correct)\n",
    "    \n",
    "    print(f\"\\nPreprocessed data date range:\")\n",
    "    print(f\"  Earliest: {dates_series.min().strftime('%d-%b-%Y %H:%M')} ({dates_series.min().strftime('%A')})\")\n",
    "    print(f\"  Latest: {dates_series.max().strftime('%d-%b-%Y %H:%M')} ({dates_series.max().strftime('%A')})\")\n",
    "    print(f\"  Total span: {(dates_series.max() - dates_series.min()).days} days\")\n",
    "    \n",
    "    # Check SWR period\n",
    "    swr_start = pd.to_datetime('2024-12-08')\n",
    "    swr_end = pd.to_datetime('2025-01-04')\n",
    "    \n",
    "    in_swr = dates_series[(dates_series >= swr_start) & (dates_series <= swr_end)]\n",
    "    \n",
    "    print(f\"\\nSWR period: 08-Dec-2024 to 04-Jan-2025\")\n",
    "    print(f\"Delay records in SWR period: {len(in_swr)}\")\n",
    "    \n",
    "    if len(in_swr) > 0:\n",
    "        print(f\"✓ SWR period IS covered in the data!\")\n",
    "        print(f\"  Range: {in_swr.min().strftime('%d-%b-%Y')} to {in_swr.max().strftime('%d-%b-%Y')}\")\n",
    "        print(f\"\\n  → Can use EXACT DATE matching for trains with delays in SWR period\")\n",
    "    else:\n",
    "        print(f\"✗ NO delay records in SWR period\")\n",
    "        if dates_series.max() < swr_start:\n",
    "            print(f\"  → Data ends BEFORE SWR period starts\")\n",
    "        elif dates_series.min() > swr_end:\n",
    "            print(f\"  → Data starts AFTER SWR period ends\")\n",
    "    \n",
    "    print(f\"\\n  → Will use WEEKDAY matching for all trains (including those without delays)\")\n",
    "    \n",
    "    # Show monthly distribution\n",
    "    print(f\"\\nMonthly distribution of delay records:\")\n",
    "    monthly = dates_series.dt.to_period('M').value_counts().sort_index()\n",
    "    for month, count in monthly.items():\n",
    "        print(f\"  {month}: {count:,} records\")\n",
    "else:\n",
    "    print(\"No valid dates found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "912b8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching SWR passenger data with preprocessed train data:\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m swr_lookup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrigin_STANOX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m swr_lookup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrigin_STANOX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     30\u001b[0m swr_lookup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination_STANOX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m swr_lookup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination_STANOX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m swr_lookup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation_STANOX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mswr_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocation_STANOX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Map WEEKDAY to day names for merging\u001b[39;00m\n\u001b[0;32m     34\u001b[0m weekday_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonday\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTuesday\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;241m6\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSunday\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     42\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\39342\\anaconda3\\envs\\rdm_env\\lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\39342\\anaconda3\\envs\\rdm_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\39342\\anaconda3\\envs\\rdm_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\39342\\anaconda3\\envs\\rdm_env\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\39342\\anaconda3\\envs\\rdm_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\39342\\anaconda3\\envs\\rdm_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\39342\\anaconda3\\envs\\rdm_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# Match and merge SWR passenger loading data with preprocessed train data (OPTIMIZED)\n",
    "print(\"Matching SWR passenger data with preprocessed train data:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Add Location_STANOX to SWR data if not already present\n",
    "if 'Location_STANOX' not in SWR_data.columns:\n",
    "    SWR_data['Location_STANOX'] = SWR_data['Location'].map(tiploc_to_stanox)\n",
    "\n",
    "# Prepare SWR data for merging\n",
    "# Create a lookup table with only the columns we need\n",
    "swr_lookup = SWR_data[[\n",
    "    'Origin_STANOX',\n",
    "    'Destination_STANOX', \n",
    "    'Location_STANOX',\n",
    "    'Calendar Day',\n",
    "    'Formation',\n",
    "    'Car Count',\n",
    "    'Standard Seated',\n",
    "    'Standing',\n",
    "    'Capacity',\n",
    "    'First Class Seats',\n",
    "    'Average Train Load on Departure',\n",
    "    '% of Seated Capacity',\n",
    "    '% of Total Capacity',\n",
    "    'Number of Readings'\n",
    "]].copy()\n",
    "\n",
    "# FIX: Convert STANOX columns to int to match preprocessed data\n",
    "swr_lookup['Origin_STANOX'] = swr_lookup['Origin_STANOX'].astype(int)\n",
    "swr_lookup['Destination_STANOX'] = swr_lookup['Destination_STANOX'].astype(int)\n",
    "swr_lookup['Location_STANOX'] = swr_lookup['Location_STANOX'].astype(int)\n",
    "\n",
    "# Map WEEKDAY to day names for merging\n",
    "weekday_map = {\n",
    "    0: 'Monday',\n",
    "    1: 'Tuesday', \n",
    "    2: 'Wednesday',\n",
    "    3: 'Thursday',\n",
    "    4: 'Friday',\n",
    "    5: 'Saturday',\n",
    "    6: 'Sunday'\n",
    "}\n",
    "\n",
    "print(f\"\\nProcessing {len(SWR_available_stations)} stations...\")\n",
    "\n",
    "# Initialize list to store all DataFrames\n",
    "all_enriched_data = []\n",
    "files_processed = 0\n",
    "total_original_records = 0\n",
    "total_matched_records = 0\n",
    "\n",
    "# Process each available SWR station\n",
    "for idx, row in SWR_available_stations.iterrows():\n",
    "    stanox = str(row['STANOX'])\n",
    "    tiploc = row['TIPLOC']\n",
    "    \n",
    "    station_dir = os.path.join(processed_data_dir, stanox)\n",
    "    \n",
    "    if os.path.exists(station_dir):\n",
    "        parquet_files = [f for f in os.listdir(station_dir) if f.endswith('.parquet')]\n",
    "        \n",
    "        for pq_file in parquet_files:\n",
    "            try:\n",
    "                # Read preprocessed data\n",
    "                preprocessed_data = pd.read_parquet(os.path.join(station_dir, pq_file), engine='fastparquet')\n",
    "                files_processed += 1\n",
    "                original_count = len(preprocessed_data)\n",
    "                total_original_records += original_count\n",
    "                \n",
    "                # Add STANOX column from folder name (as int)\n",
    "                preprocessed_data['STANOX'] = int(stanox)\n",
    "                preprocessed_data['TIPLOC'] = tiploc\n",
    "                \n",
    "                # Check required columns\n",
    "                if all(col in preprocessed_data.columns for col in \n",
    "                       ['PLANNED_ORIGIN_LOCATION_CODE', 'PLANNED_DEST_LOCATION_CODE', 'WEEKDAY']):\n",
    "                    \n",
    "                    # Map WEEKDAY to day names\n",
    "                    preprocessed_data['DayName'] = preprocessed_data['WEEKDAY'].map(weekday_map)\n",
    "                    \n",
    "                    # OPTIMIZED: Use pandas merge instead of row-by-row matching\n",
    "                    # Merge on: Origin, Destination, Location, and Day of Week\n",
    "                    enriched = preprocessed_data.merge(\n",
    "                        swr_lookup,\n",
    "                        left_on=['PLANNED_ORIGIN_LOCATION_CODE', 'PLANNED_DEST_LOCATION_CODE', 'STANOX', 'DayName'],\n",
    "                        right_on=['Origin_STANOX', 'Destination_STANOX', 'Location_STANOX', 'Calendar Day'],\n",
    "                        how='inner'  # Only keep matches\n",
    "                    )\n",
    "                    \n",
    "                    # Remove duplicate merge columns\n",
    "                    enriched = enriched.drop(columns=['Origin_STANOX', 'Destination_STANOX', 'Location_STANOX', 'Calendar Day'], errors='ignore')\n",
    "                    \n",
    "                    if len(enriched) > 0:\n",
    "                        # If there are duplicate matches (multiple SWR records for same train),\n",
    "                        # group by the train's original columns and take mean of SWR columns\n",
    "                        train_id_cols = [col for col in preprocessed_data.columns if col in enriched.columns]\n",
    "                        swr_numeric_cols = ['Car Count', 'Standard Seated', 'Standing', 'Capacity', \n",
    "                                           'First Class Seats', 'Average Train Load on Departure',\n",
    "                                           '% of Seated Capacity', '% of Total Capacity', 'Number of Readings']\n",
    "                        \n",
    "                        # Aggregate duplicates\n",
    "                        agg_dict = {}\n",
    "                        for col in enriched.columns:\n",
    "                            if col in swr_numeric_cols:\n",
    "                                agg_dict[col] = 'mean'\n",
    "                            elif col == 'Formation':\n",
    "                                agg_dict[col] = 'first'\n",
    "                            elif col not in train_id_cols:\n",
    "                                continue\n",
    "                        \n",
    "                        if agg_dict:\n",
    "                            enriched = enriched.groupby(train_id_cols, as_index=False).agg(agg_dict)\n",
    "                        \n",
    "                        all_enriched_data.append(enriched)\n",
    "                        total_matched_records += len(enriched)\n",
    "                \n",
    "                # Print progress every 50 files\n",
    "                if files_processed % 50 == 0:\n",
    "                    print(f\"  Processed {files_processed} files, {total_original_records:,} trains, {total_matched_records:,} matched...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {pq_file} for {tiploc}: {e}\")\n",
    "\n",
    "# Combine all enriched DataFrames\n",
    "print(f\"\\nCombining all enriched data...\")\n",
    "if all_enriched_data:\n",
    "    enriched_trains_df = pd.concat(all_enriched_data, ignore_index=True)\n",
    "else:\n",
    "    enriched_trains_df = pd.DataFrame()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Matching Summary:\")\n",
    "print(f\"  Files processed: {files_processed}\")\n",
    "print(f\"  Total trains in preprocessed data: {total_original_records:,}\")\n",
    "print(f\"  Trains matched with SWR data: {total_matched_records:,}\")\n",
    "if total_original_records > 0:\n",
    "    print(f\"  Match rate: {total_matched_records/total_original_records*100:.2f}%\")\n",
    "\n",
    "if len(enriched_trains_df) > 0:\n",
    "    print(f\"\\nEnriched Dataset Info:\")\n",
    "    print(f\"  Total columns: {len(enriched_trains_df.columns)}\")\n",
    "    print(f\"  Total rows: {len(enriched_trains_df):,}\")\n",
    "    print(f\"  Memory usage: {enriched_trains_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Check SWR columns were added\n",
    "    swr_columns_to_check = ['Formation', 'Car Count', 'Standard Seated', 'Standing', 'Capacity',\n",
    "                            'First Class Seats', 'Average Train Load on Departure', \n",
    "                            '% of Seated Capacity', '% of Total Capacity', 'Number of Readings']\n",
    "    added_cols = [col for col in swr_columns_to_check if col in enriched_trains_df.columns]\n",
    "    print(f\"\\n  SWR columns successfully added: {len(added_cols)}/{len(swr_columns_to_check)}\")\n",
    "    for col in added_cols:\n",
    "        print(f\"    - {col}\")\n",
    "    \n",
    "    # Show sample statistics\n",
    "    print(f\"\\nSample Passenger Loading Statistics:\")\n",
    "    if 'Average Train Load on Departure' in enriched_trains_df.columns:\n",
    "        print(f\"  Average train load: {enriched_trains_df['Average Train Load on Departure'].mean():.1f} passengers\")\n",
    "        print(f\"  Min train load: {enriched_trains_df['Average Train Load on Departure'].min():.1f}\")\n",
    "        print(f\"  Max train load: {enriched_trains_df['Average Train Load on Departure'].max():.1f}\")\n",
    "    \n",
    "    if '% of Seated Capacity' in enriched_trains_df.columns:\n",
    "        print(f\"  Average % seated capacity: {enriched_trains_df['% of Seated Capacity'].mean():.2%}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nSample of enriched data (first 5 rows):\")\n",
    "    display_cols = ['TIPLOC', 'PLANNED_ORIGIN_LOCATION_CODE', 'PLANNED_DEST_LOCATION_CODE', \n",
    "                    'DayName', 'Average Train Load on Departure', '% of Seated Capacity', 'Capacity']\n",
    "    available_display_cols = [col for col in display_cols if col in enriched_trains_df.columns]\n",
    "    print(enriched_trains_df[available_display_cols].head().to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n✓ Enriched dataset created successfully as 'enriched_trains_df'\")\n",
    "else:\n",
    "    print(\"\\n✗ No trains were matched with SWR data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
